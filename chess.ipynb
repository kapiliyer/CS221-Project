{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba45df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import chess\n",
    "\n",
    "PATH = 'data/chessData.csv'\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1721418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH, encoding=\"utf-8\")\n",
    "# 100k datapoints. White's turn. Preprocess to remove \"mate in X\" evaluations with '#' character. All values between -2k and 2k centipawns. No 0-evaluation.\n",
    "df['Evaluation'] = df['Evaluation'].apply(pd.to_numeric, errors='coerce')\n",
    "df = df[(-2000 <= df['Evaluation']) & (df['Evaluation'] <= 2000) & (df['Evaluation'] != 0) & (df['FEN'].apply(lambda fen: fen.split()[1]) == 'w')].dropna()[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2743de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Evaluation']\n",
    "fig = y.plot.hist(bins=100)\n",
    "fig.set_title('Distribution of Evaluation Scores over Dataset')\n",
    "fig.set_xlabel('Evaluation Score (Centipawns)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa70702",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 80/10/10 train/val/test split\n",
    "df_train, df_test  = train_test_split(df, test_size=0.2, random_state=1)\n",
    "df_val, df_test= train_test_split(df_test, test_size=0.5, random_state=1)\n",
    "print('Train/Validation/Test Splits: ' , df_train.shape[0], df_val.shape[0], df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ece20c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Credits to @shailesh for portions of this code.\n",
    "https://github.com/ShaileshSridhar2403/neuralStockfish\n",
    "\"\"\"\n",
    "pieces = (chess.PAWN,chess.KNIGHT,chess.BISHOP,chess.ROOK,chess.QUEEN,chess.KING)\n",
    "colours = (chess.WHITE,chess.BLACK)\n",
    "\n",
    "def fenToVec(fen):\n",
    "\t\"\"\"\n",
    "\tInput: \n",
    "\tFEN string.\n",
    "\t\n",
    "\tOutput: 768-vector. Each of the 12 sets (where each set is one of the unique pieces) \n",
    "\tof 64 elements (where each element is a square) is a one-hot encoding of whether the\n",
    "\tpiece is on the square. 12 * 64 = 768.\n",
    "\t\"\"\"\n",
    "\tposFen = fen.split()[0]\n",
    "\tboard = chess.BaseBoard(posFen)\n",
    "\tl = []\n",
    "\t\n",
    "\tfor colour in colours:\n",
    "\t\tfor piece in pieces:\n",
    "\t\t\tv = np.zeros(64)\n",
    "\t\t\tfor i in board.pieces(piece,colour):\n",
    "\t\t\t\tv[i] = 1\n",
    "\t\t\tl.append(v)\n",
    "\tl = np.concatenate(l)\n",
    "\treturn l\n",
    "\n",
    "\n",
    "def vecToFen(vec):\n",
    "\t\"\"\"\n",
    "\tReverses above function.\n",
    "\t\"\"\"\n",
    "\tvecList = np.split(vec,12)\n",
    "\twhiteList = vecList[:6]\n",
    "\tblackList = vecList[6:]\n",
    "\tboard = chess.BaseBoard()\n",
    "\tboard.clear_board()\n",
    "\tfor pieceType in range(len(whiteList)):\n",
    "\t\tpieceArr = whiteList[pieceType]\n",
    "\t\tfor ind in range(len(pieceArr)):\n",
    "\t\t\tif pieceArr[ind]:\n",
    "\t\t\t\tboard.set_piece_at(ind ,chess.Piece(pieces[pieceType],chess.WHITE))\n",
    "\t\t\t\t\n",
    "\tfor pieceType in range(len(blackList)):\n",
    "\t\tpieceArr = blackList[pieceType]\n",
    "\t\tfor ind in range(len(pieceArr)):\n",
    "\t\t\tif pieceArr[ind]:\n",
    "\t\t\t\tboard.set_piece_at(ind ,chess.Piece(pieces[pieceType],chess.BLACK))\n",
    "\t\n",
    "\treturn board.board_fen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac0c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fenToVec2D(x):\n",
    "    return pd.DataFrame(x.apply(fenToVec).values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb7a9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel:\n",
    "    def __init__(self):\n",
    "        # Value of pieces in centipawns.\n",
    "        self.values = {chess.PAWN:100, chess.KNIGHT:300, chess.BISHOP:300, chess.ROOK:500, chess.QUEEN:900, chess.KING:0}\n",
    "\n",
    "    def predict(self, x):\n",
    "        board = chess.BaseBoard(x.split()[0])\n",
    "        pred = sum((1 if piece.color == chess.WHITE else -1) * self.values[piece.piece_type] for piece in board.piece_map().values())\n",
    "        return pred if pred else random.uniform(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f29664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(y, y_pred):\n",
    "    \"\"\"\n",
    "    Returns MSE of evaluation scores and accuracy of sign of evaluation \n",
    "    scores (which indicate winning player).\n",
    "    \"\"\"\n",
    "    mse = np.mean((y - y_pred)**2)\n",
    "    accuracy = np.mean(np.sign(y) == np.sign(y_pred))\n",
    "    return mse, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb373829",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Baseline Test\n",
    "baseline = BaselineModel()\n",
    "\n",
    "y_pred = df_train['FEN'].apply(baseline.predict)\n",
    "mse, accuracy = eval(df_train['Evaluation'], y_pred)\n",
    "print(f'Baseline Train Error: {mse}.')\n",
    "print(f'Baseline Train Accuracy: {accuracy}.')\n",
    "\n",
    "y_pred = df_val['FEN'].apply(baseline.predict)\n",
    "mse, accuracy = eval(df_val['Evaluation'], y_pred)\n",
    "print(f'Baseline Validation Error: {mse}.')\n",
    "print(f'Baseline Validation Accuracy: {accuracy}.')\n",
    "\n",
    "y_pred = df_test['FEN'].apply(baseline.predict)\n",
    "mse, accuracy = eval(df_test['Evaluation'], y_pred)\n",
    "print(f'Baseline Test Error: {mse}.')\n",
    "print(f'Baseline Test Accuracy: {accuracy}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be954bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPModel, self).__init__()\n",
    "        DROPOUT_PROB = 0.5\n",
    "        INPUT_SIZE = 768\n",
    "        self.dropout = nn.Dropout(DROPOUT_PROB)\n",
    "        self.linear1 = nn.Linear(INPUT_SIZE, INPUT_SIZE // 2)\n",
    "        self.linear2 = nn.Linear(INPUT_SIZE // 2, INPUT_SIZE // 4)\n",
    "        self.linear3 = nn.Linear(INPUT_SIZE // 4, 1)\n",
    "\n",
    "    def predict(self, x, device):\n",
    "        input = torch.as_tensor(fenToVec2D(x).values, dtype=torch.float32)\n",
    "        input = input.to(device)\n",
    "        input = F.relu(self.linear1(input))\n",
    "        input = self.dropout(input)\n",
    "        input = F.relu(self.linear2(input))\n",
    "        input = self.dropout(input)\n",
    "        output = self.linear3(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85edfaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        DROPOUT_PROB = 0.5\n",
    "        INPUT_SIZE = 128\n",
    "        self.dropout = nn.Dropout(DROPOUT_PROB)\n",
    "        self.linear1 = nn.Linear(INPUT_SIZE, INPUT_SIZE // 2)\n",
    "        self.linear2 = nn.Linear(INPUT_SIZE // 2, INPUT_SIZE // 4)\n",
    "        self.linear3 = nn.Linear(INPUT_SIZE // 4, 1)\n",
    "        self.conv1 = nn.Conv2d(in_channels = 12, out_channels = 32, kernel_size = 3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels = 64, out_channels = INPUT_SIZE, kernel_size = 3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def predict(self, x, device):\n",
    "        input = torch.as_tensor(fenToVec2D(x).values, dtype=torch.float32)\n",
    "        input = input.to(device)\n",
    "        input = input.reshape(-1, 12, 8, 8)\n",
    "        input = self.pool(self.conv1(input))\n",
    "        input = self.pool(self.conv2(input))\n",
    "        input = self.dropout(self.pool(self.conv3(input)))\n",
    "        input = torch.flatten(input, start_dim=1)   \n",
    "        input = F.relu(self.linear1(input))\n",
    "        input = self.dropout(input)\n",
    "        input = F.relu(self.linear2(input))\n",
    "        input = self.dropout(input)\n",
    "        output = self.linear3(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5470997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "def batches(dataset, sz=256):\n",
    "    return enumerate(dataset[i*sz:(i+1)*sz] for i in range(ceil(dataset.shape[0] / sz)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a3cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def test(model, device, dataset, flag):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        truth = []\n",
    "        predictions = []\n",
    "        for step, batch in tqdm(batches(dataset), desc=f\"{flag} eval\"):\n",
    "            b_fens, b_labels = batch['FEN'], batch['Evaluation']\n",
    "            truth.extend(b_labels)\n",
    "            logits = model.predict(b_fens, device)\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            predictions.extend(logits)\n",
    "        mse, accuracy = eval(np.array(truth).flatten(), np.array(predictions).flatten())\n",
    "    return mse, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eeb97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def train(modelType, trainset, valset, testset, lr, epochs):\n",
    "    print(f\"Beginning training with {epochs} epochs.\")\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "    model = modelType()\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr) #weight decay 0.01\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "    \n",
    "    train_mse, train_acc = test(model, device, trainset, 'train')\n",
    "    val_mse, val_acc = test(model, device, valset, 'val')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"epoch number: {epoch}, train acc: {train_acc}, val acc: {val_acc}, train mse: {train_mse}, val mse: {val_mse}\")\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in tqdm(batches(trainset), desc='train'):\n",
    "            b_fens, b_labels = batch['FEN'], batch['Evaluation']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits = model.predict(b_fens, device)\n",
    "\n",
    "            b_labels = torch.as_tensor(b_labels.values).float()\n",
    "            b_labels = b_labels.to(device)\n",
    "\n",
    "            loss = F.mse_loss(logits.flatten(), b_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_mse, train_acc = test(model, device, trainset, 'train')\n",
    "        val_mse, val_acc = test(model, device, valset, 'val')\n",
    "        \n",
    "        scheduler.step(val_mse)\n",
    "    \n",
    "    test_mse, test_acc = test(model, device, testset, 'test')\n",
    "    print(f\"epoch number: {epochs}, train acc: {train_acc}, val acc: {val_acc}, test acc: {test_acc}, train mse: {train_mse}, val mse: {val_mse}, test mse: {test_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50ddc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MLP Test\n",
    "train(MLPModel, df_train, df_val, df_test, 3e-3, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeb2c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CNN Test\n",
    "trained_model = train(CNNModel, df_train, df_val, df_test, 3e-4, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fd5d45a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess.svg\n",
    "\n",
    "def experimentation(model, examples):\n",
    "    \"\"\"\n",
    "    Examples is a df where each row contains a FEN and Evaluation score.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for example in examples.iterrows():\n",
    "            board = chess.BaseBoard(example['FEN'])\n",
    "            chess.svg.board(board, squares=chess.SquareSet(chess.BB_DARK_SQUARES & chess.BB_FILE_B), size=350) # displays image of board\n",
    "            print(f\"Model Eval: {model.predict(example['FEN'], torch.device('cpu'))}\")\n",
    "            print(f\"Stockfish Eval: {example['Evaluation']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
